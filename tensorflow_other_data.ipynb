{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.6)\n",
    "X_train, X_test = X_train.astype(np.float32), X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(keras.Model):\n",
    "    def __init__(self, hidden_size=5, output_size=3, name = \"mynn\"):\n",
    "        super().__init__(name = name)\n",
    "        self.l1 = layers.Dense(hidden_size)\n",
    "        self.l2 = layers.Dense(output_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = keras.activations.sigmoid(y)\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN(5, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1456 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.9167\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1366 - acc: 1.0000 - val_loss: 0.2108 - val_acc: 0.9167\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1344 - acc: 1.0000 - val_loss: 0.2105 - val_acc: 0.9167\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1295 - acc: 1.0000 - val_loss: 0.2061 - val_acc: 0.9167\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1292 - acc: 1.0000 - val_loss: 0.2029 - val_acc: 0.9167\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1238 - acc: 1.0000 - val_loss: 0.2019 - val_acc: 0.9167\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1216 - acc: 1.0000 - val_loss: 0.2029 - val_acc: 0.9167\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1198 - acc: 1.0000 - val_loss: 0.1968 - val_acc: 0.9167\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1153 - acc: 1.0000 - val_loss: 0.1941 - val_acc: 0.9167\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1144 - acc: 1.0000 - val_loss: 0.1901 - val_acc: 0.9167\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1115 - acc: 1.0000 - val_loss: 0.1896 - val_acc: 0.9167\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1084 - acc: 1.0000 - val_loss: 0.1896 - val_acc: 0.9167\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1065 - acc: 1.0000 - val_loss: 0.1880 - val_acc: 0.9167\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1058 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9167\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1023 - acc: 1.0000 - val_loss: 0.1856 - val_acc: 0.9167\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1001 - acc: 1.0000 - val_loss: 0.1839 - val_acc: 0.9167\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0992 - acc: 1.0000 - val_loss: 0.1790 - val_acc: 0.9167\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0974 - acc: 1.0000 - val_loss: 0.1800 - val_acc: 0.9167\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0955 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.9167\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0938 - acc: 1.0000 - val_loss: 0.1791 - val_acc: 0.9167\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0924 - acc: 1.0000 - val_loss: 0.1740 - val_acc: 0.9167\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0903 - acc: 1.0000 - val_loss: 0.1737 - val_acc: 0.9167\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0907 - acc: 1.0000 - val_loss: 0.1691 - val_acc: 0.9167\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0851 - acc: 1.0000 - val_loss: 0.1725 - val_acc: 0.9167\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0831 - acc: 1.0000 - val_loss: 0.1700 - val_acc: 0.9167\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0801 - acc: 1.0000 - val_loss: 0.1695 - val_acc: 0.9167\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0754 - acc: 1.0000 - val_loss: 0.1629 - val_acc: 0.9167\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0733 - acc: 1.0000 - val_loss: 0.1545 - val_acc: 0.9167\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0728 - acc: 1.0000 - val_loss: 0.1595 - val_acc: 0.9167\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0696 - acc: 1.0000 - val_loss: 0.1582 - val_acc: 0.9167\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0676 - acc: 1.0000 - val_loss: 0.1623 - val_acc: 0.9167\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0679 - acc: 1.0000 - val_loss: 0.1672 - val_acc: 0.9167\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.1538 - val_acc: 0.9167\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0634 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 0.9167\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0619 - acc: 1.0000 - val_loss: 0.1567 - val_acc: 0.9167\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0625 - acc: 1.0000 - val_loss: 0.1622 - val_acc: 0.9167\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0617 - acc: 1.0000 - val_loss: 0.1560 - val_acc: 0.9167\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0602 - acc: 1.0000 - val_loss: 0.1469 - val_acc: 0.9167\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0591 - acc: 1.0000 - val_loss: 0.1527 - val_acc: 0.9167\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0573 - acc: 1.0000 - val_loss: 0.1567 - val_acc: 0.9167\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0561 - acc: 1.0000 - val_loss: 0.1585 - val_acc: 0.9167\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0555 - acc: 1.0000 - val_loss: 0.1545 - val_acc: 0.9167\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0556 - acc: 1.0000 - val_loss: 0.1471 - val_acc: 0.9167\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0552 - acc: 1.0000 - val_loss: 0.1466 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0530 - acc: 1.0000 - val_loss: 0.1586 - val_acc: 0.9167\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0529 - acc: 1.0000 - val_loss: 0.1536 - val_acc: 0.9167\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0523 - acc: 1.0000 - val_loss: 0.1483 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0502 - acc: 1.0000 - val_loss: 0.1551 - val_acc: 0.9167\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.1518 - val_acc: 0.9167\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0517 - acc: 1.0000 - val_loss: 0.1420 - val_acc: 0.9167\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.1475 - val_acc: 0.9167\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.1608 - val_acc: 0.9167\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0480 - acc: 1.0000 - val_loss: 0.1485 - val_acc: 0.9167\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0466 - acc: 1.0000 - val_loss: 0.1441 - val_acc: 0.9167\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0487 - acc: 1.0000 - val_loss: 0.1551 - val_acc: 0.9167\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0459 - acc: 1.0000 - val_loss: 0.1484 - val_acc: 0.9167\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0478 - acc: 1.0000 - val_loss: 0.1395 - val_acc: 0.9167\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0457 - acc: 1.0000 - val_loss: 0.1511 - val_acc: 0.9167\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0435 - acc: 1.0000 - val_loss: 0.1499 - val_acc: 0.9167\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0436 - acc: 1.0000 - val_loss: 0.1438 - val_acc: 0.9167\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0446 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9167\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0414 - acc: 1.0000 - val_loss: 0.1585 - val_acc: 0.9167\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0448 - acc: 1.0000 - val_loss: 0.1481 - val_acc: 0.9167\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.1495 - val_acc: 0.9167\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.1469 - val_acc: 0.9167\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0456 - acc: 1.0000 - val_loss: 0.1361 - val_acc: 0.9167\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0415 - acc: 1.0000 - val_loss: 0.1544 - val_acc: 0.9167\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.1587 - val_acc: 0.9167\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.1461 - val_acc: 0.9167\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.1373 - val_acc: 0.9167\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.1493 - val_acc: 0.9167\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.1519 - val_acc: 0.9167\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.1449 - val_acc: 0.9167\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.1508 - val_acc: 0.9167\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.1434 - val_acc: 0.9167\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.1472 - val_acc: 0.9167\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0369 - acc: 1.0000 - val_loss: 0.1463 - val_acc: 0.9167\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.1524 - val_acc: 0.9167\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.1392 - val_acc: 0.9167\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.1349 - val_acc: 0.9167\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0331 - acc: 1.0000 - val_loss: 0.1551 - val_acc: 0.9167\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.1593 - val_acc: 0.9167\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.1555 - val_acc: 0.9167\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.1350 - val_acc: 0.9167\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0337 - acc: 1.0000 - val_loss: 0.1427 - val_acc: 0.9167\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0334 - acc: 1.0000 - val_loss: 0.1540 - val_acc: 0.9167\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9167\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.1407 - val_acc: 0.9167\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.1431 - val_acc: 0.9167\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0315 - acc: 1.0000 - val_loss: 0.1472 - val_acc: 0.9167\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.1514 - val_acc: 0.9167\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.1504 - val_acc: 0.9167\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.1452 - val_acc: 0.9167\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.1507 - val_acc: 0.9167\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.1408 - val_acc: 0.9167\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.1468 - val_acc: 0.9167\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.1418 - val_acc: 0.9167\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.1491 - val_acc: 0.9167\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.1477 - val_acc: 0.9167\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.1495 - val_acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.01),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = \"acc\",\n",
    "    run_eagerly = False\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 10, epochs = 100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe49d6aa850>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df =  pd.DataFrame(history.history)\n",
    "plt.plot(df[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0673 - acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06728243827819824, 0.9777777791023254]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size = 0.6)\n",
    "X_train = prepro.scale(X_train)\n",
    "X_test = prepro.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.3267 - acc: 0.3393 - val_loss: 1.1923 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0832 - acc: 0.4286 - val_loss: 1.0928 - val_acc: 0.4000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1657 - acc: 0.3929 - val_loss: 1.1977 - val_acc: 0.4000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2007 - acc: 0.3929 - val_loss: 1.0816 - val_acc: 0.4000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0701 - acc: 0.3929 - val_loss: 1.0024 - val_acc: 0.4000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9975 - acc: 0.3929 - val_loss: 1.0117 - val_acc: 0.3333\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0125 - acc: 0.3571 - val_loss: 1.0162 - val_acc: 0.3333\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0027 - acc: 0.3571 - val_loss: 0.9681 - val_acc: 0.3333\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.9544 - acc: 0.3571 - val_loss: 0.9291 - val_acc: 0.4000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9402 - acc: 0.3929 - val_loss: 0.9235 - val_acc: 0.4000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.9234 - acc: 0.3929 - val_loss: 0.8996 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8985 - acc: 0.3929 - val_loss: 0.8732 - val_acc: 0.4667\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8667 - acc: 0.4107 - val_loss: 0.8575 - val_acc: 0.4667\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8594 - acc: 0.4107 - val_loss: 0.8564 - val_acc: 0.4000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8546 - acc: 0.5000 - val_loss: 0.8389 - val_acc: 0.5333\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8345 - acc: 0.5000 - val_loss: 0.8167 - val_acc: 0.6000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8147 - acc: 0.5357 - val_loss: 0.8082 - val_acc: 0.6000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8115 - acc: 0.5536 - val_loss: 0.8041 - val_acc: 0.6667\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8034 - acc: 0.5893 - val_loss: 0.7856 - val_acc: 0.6667\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7835 - acc: 0.6071 - val_loss: 0.7742 - val_acc: 0.6667\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7731 - acc: 0.6250 - val_loss: 0.7683 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7700 - acc: 0.5179 - val_loss: 0.7601 - val_acc: 0.6000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7603 - acc: 0.6071 - val_loss: 0.7517 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7511 - acc: 0.6607 - val_loss: 0.7429 - val_acc: 0.6667\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7425 - acc: 0.6429 - val_loss: 0.7377 - val_acc: 0.6667\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7432 - acc: 0.6429 - val_loss: 0.7311 - val_acc: 0.6667\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7319 - acc: 0.6429 - val_loss: 0.7270 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7321 - acc: 0.6429 - val_loss: 0.7193 - val_acc: 0.6667\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7189 - acc: 0.6250 - val_loss: 0.7146 - val_acc: 0.6667\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7148 - acc: 0.6607 - val_loss: 0.7119 - val_acc: 0.6000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7160 - acc: 0.5357 - val_loss: 0.7028 - val_acc: 0.6667\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7023 - acc: 0.6071 - val_loss: 0.6968 - val_acc: 0.6667\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7043 - acc: 0.6429 - val_loss: 0.6934 - val_acc: 0.6667\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6928 - acc: 0.6429 - val_loss: 0.6875 - val_acc: 0.6667\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6892 - acc: 0.6429 - val_loss: 0.6829 - val_acc: 0.6667\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6867 - acc: 0.6429 - val_loss: 0.6810 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6810 - acc: 0.6964 - val_loss: 0.6750 - val_acc: 0.6000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6745 - acc: 0.6250 - val_loss: 0.6706 - val_acc: 0.6667\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6762 - acc: 0.6429 - val_loss: 0.6711 - val_acc: 0.6667\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6706 - acc: 0.6429 - val_loss: 0.6639 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6618 - acc: 0.6429 - val_loss: 0.6593 - val_acc: 0.6000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6669 - acc: 0.5536 - val_loss: 0.6607 - val_acc: 0.6000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6598 - acc: 0.6964 - val_loss: 0.6524 - val_acc: 0.6667\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6514 - acc: 0.6429 - val_loss: 0.6495 - val_acc: 0.6667\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6494 - acc: 0.6429 - val_loss: 0.6483 - val_acc: 0.6667\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6501 - acc: 0.6429 - val_loss: 0.6428 - val_acc: 0.6667\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6407 - acc: 0.6429 - val_loss: 0.6384 - val_acc: 0.6000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6387 - acc: 0.6786 - val_loss: 0.6412 - val_acc: 0.6667\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6446 - acc: 0.6786 - val_loss: 0.6359 - val_acc: 0.7333\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6392 - acc: 0.6607 - val_loss: 0.6290 - val_acc: 0.6667\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6293 - acc: 0.6429 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6323 - acc: 0.6429 - val_loss: 0.6232 - val_acc: 0.6667\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6209 - acc: 0.6429 - val_loss: 0.6203 - val_acc: 0.6667\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6188 - acc: 0.6607 - val_loss: 0.6225 - val_acc: 0.7333\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6217 - acc: 0.7857 - val_loss: 0.6190 - val_acc: 0.7333\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6272 - acc: 0.6607 - val_loss: 0.6098 - val_acc: 0.6667\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6101 - acc: 0.6429 - val_loss: 0.6066 - val_acc: 0.6667\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6107 - acc: 0.6429 - val_loss: 0.6036 - val_acc: 0.6667\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6035 - acc: 0.6250 - val_loss: 0.6025 - val_acc: 0.7333\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6037 - acc: 0.6786 - val_loss: 0.6011 - val_acc: 0.6667\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6018 - acc: 0.6250 - val_loss: 0.5947 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5989 - acc: 0.6429 - val_loss: 0.5915 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6066 - acc: 0.6607 - val_loss: 0.5898 - val_acc: 0.7333\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5898 - acc: 0.6786 - val_loss: 0.5855 - val_acc: 0.7333\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5867 - acc: 0.6429 - val_loss: 0.5828 - val_acc: 0.6667\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5866 - acc: 0.6429 - val_loss: 0.5789 - val_acc: 0.6667\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5801 - acc: 0.6429 - val_loss: 0.5759 - val_acc: 0.7333\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5783 - acc: 0.6429 - val_loss: 0.5739 - val_acc: 0.7333\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5722 - acc: 0.6786 - val_loss: 0.5753 - val_acc: 0.8667\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5754 - acc: 0.7321 - val_loss: 0.5762 - val_acc: 0.8000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5707 - acc: 0.7679 - val_loss: 0.5649 - val_acc: 0.8000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5659 - acc: 0.6607 - val_loss: 0.5596 - val_acc: 0.7333\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5617 - acc: 0.6429 - val_loss: 0.5565 - val_acc: 0.8000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5600 - acc: 0.6607 - val_loss: 0.5557 - val_acc: 0.8000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5535 - acc: 0.7143 - val_loss: 0.5531 - val_acc: 0.8667\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5497 - acc: 0.7143 - val_loss: 0.5514 - val_acc: 0.8667\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5504 - acc: 0.7321 - val_loss: 0.5514 - val_acc: 0.8667\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5426 - acc: 0.7321 - val_loss: 0.5437 - val_acc: 0.8667\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5512 - acc: 0.7143 - val_loss: 0.5383 - val_acc: 0.8000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5368 - acc: 0.6964 - val_loss: 0.5379 - val_acc: 0.8667\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5346 - acc: 0.7321 - val_loss: 0.5438 - val_acc: 0.8667\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5293 - acc: 0.8036 - val_loss: 0.5392 - val_acc: 0.8667\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5259 - acc: 0.7500 - val_loss: 0.5290 - val_acc: 0.8667\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5188 - acc: 0.7321 - val_loss: 0.5243 - val_acc: 0.8667\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5193 - acc: 0.7143 - val_loss: 0.5178 - val_acc: 0.8667\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5131 - acc: 0.7500 - val_loss: 0.5171 - val_acc: 0.8667\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5045 - acc: 0.8036 - val_loss: 0.5133 - val_acc: 0.8667\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4946 - acc: 0.8214 - val_loss: 0.5031 - val_acc: 0.8667\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4850 - acc: 0.8036 - val_loss: 0.4903 - val_acc: 0.8667\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4787 - acc: 0.7857 - val_loss: 0.4791 - val_acc: 0.8667\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4653 - acc: 0.7857 - val_loss: 0.4645 - val_acc: 0.9333\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4514 - acc: 0.8393 - val_loss: 0.4509 - val_acc: 0.9333\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4338 - acc: 0.8571 - val_loss: 0.4351 - val_acc: 0.9333\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4181 - acc: 0.9286 - val_loss: 0.4213 - val_acc: 0.8667\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3946 - acc: 0.9286 - val_loss: 0.3905 - val_acc: 0.8667\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3703 - acc: 0.9286 - val_loss: 0.3593 - val_acc: 0.8667\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3411 - acc: 0.9286 - val_loss: 0.3247 - val_acc: 0.9333\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3185 - acc: 0.9286 - val_loss: 0.2939 - val_acc: 0.9333\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2931 - acc: 0.9286 - val_loss: 0.2682 - val_acc: 0.9333\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2667 - acc: 0.9643 - val_loss: 0.2504 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe49e92c9d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adamax(learning_rate = 0.02),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics=\"acc\",\n",
    "    run_eagerly=False\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 763us/step - loss: 0.2907 - acc: 0.9439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29068461060523987, 0.9439252614974976]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Configures the model for training.\n",
       "\n",
       "Arguments:\n",
       "    optimizer: String (name of optimizer) or optimizer instance. See\n",
       "      `tf.keras.optimizers`.\n",
       "    loss: String (name of objective function), objective function or\n",
       "      `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
       "      function is any callable with the signature `loss = fn(y_true,\n",
       "      y_pred)`, where y_true = ground truth values with shape =\n",
       "      `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
       "      categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
       "      y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
       "      returns a weighted loss float tensor. If a custom `Loss` instance is\n",
       "      used and reduction is set to NONE, return value has the shape\n",
       "      [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
       "      otherwise, it is a scalar. If the model has multiple outputs, you can\n",
       "      use a different loss on each output by passing a dictionary or a list\n",
       "      of losses. The loss value that will be minimized by the model will\n",
       "      then be the sum of all individual losses.\n",
       "    metrics: List of metrics to be evaluated by the model during training\n",
       "      and testing. Each of this can be a string (name of a built-in\n",
       "      function), function or a `tf.keras.metrics.Metric` instance. See\n",
       "      `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
       "      function is any callable with the signature `result = fn(y_true,\n",
       "      y_pred)`. To specify different metrics for different outputs of a\n",
       "      multi-output model, you could also pass a dictionary, such as\n",
       "        `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
       "          You can also pass a list (len = len(outputs)) of lists of metrics\n",
       "          such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
       "          `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
       "          strings 'accuracy' or 'acc', we convert this to one of\n",
       "          `tf.keras.metrics.BinaryAccuracy`,\n",
       "          `tf.keras.metrics.CategoricalAccuracy`,\n",
       "          `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
       "          function used and the model output shape. We do a similar\n",
       "          conversion for the strings 'crossentropy' and 'ce' as well.\n",
       "    loss_weights: Optional list or dictionary specifying scalar coefficients\n",
       "      (Python floats) to weight the loss contributions of different model\n",
       "      outputs. The loss value that will be minimized by the model will then\n",
       "      be the *weighted sum* of all individual losses, weighted by the\n",
       "      `loss_weights` coefficients.\n",
       "        If a list, it is expected to have a 1:1 mapping to the model's\n",
       "          outputs. If a dict, it is expected to map output names (strings)\n",
       "          to scalar coefficients.\n",
       "    weighted_metrics: List of metrics to be evaluated and weighted by\n",
       "      sample_weight or class_weight during training and testing.\n",
       "    run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
       "      logic will not be wrapped in a `tf.function`. Recommended to leave\n",
       "      this as `None` unless your `Model` cannot be run inside a\n",
       "      `tf.function`.\n",
       "    **kwargs: Any additional arguments. Supported arguments:\n",
       "        - `experimental_steps_per_execution`: Int. The number of batches to\n",
       "          run during each `tf.function` call. Running multiple batches\n",
       "          inside a single `tf.function` call can greatly improve performance\n",
       "          on TPUs or small models with a large Python overhead. Note that if\n",
       "          this value is set to `N`, `Callback.on_batch` methods will only be\n",
       "          called every `N` batches. This currently defaults to `1`. At most,\n",
       "          one full epoch will be run each execution. If a number larger than\n",
       "          the size of the epoch is passed, the execution will be truncated\n",
       "          to the size of the epoch.\n",
       "        - `sample_weight_mode` for backward compatibility.\n",
       "\n",
       "Raises:\n",
       "    ValueError: In case of invalid arguments for\n",
       "        `optimizer`, `loss` or `metrics`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras.Model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
